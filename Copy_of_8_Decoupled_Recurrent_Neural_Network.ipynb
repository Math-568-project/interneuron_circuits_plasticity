{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of 8 Decoupled Recurrent Neural Network ",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/k47h4/interneuron_circuits_plasticity/blob/master/Copy_of_8_Decoupled_Recurrent_Neural_Network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r67KQjKySccT",
        "colab_type": "code",
        "outputId": "9d6327fe-fca9-40b5-c697-1b7fb7b3cfe7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import numpy as np,sys\n",
        "from sklearn.utils import shuffle\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "np.random.seed(678)\n",
        "\n",
        "def log(x):\n",
        "    return 1 / (1 + np.exp(-1 * x))\n",
        "def d_log(x):\n",
        "    return log(x) * ( 1 - log(x))\n",
        "\n",
        "def tanh(x):\n",
        "    return np.tanh(x)\n",
        "def d_tanh(x):\n",
        "    return 1 - np.tanh(x) ** 2 \n",
        "\n",
        "def ReLu(x):\n",
        "    mask = (x > 0.0) * 1.0\n",
        "    return x * mask\n",
        "def d_ReLu(x):\n",
        "    mask = (x > 0.0) * 1.0\n",
        "    return mask    \n",
        "\n",
        "\n",
        "# 0. Declare Training Data and Labels\n",
        "mnist = input_data.read_data_sets(\"../../MNIST_data/\", one_hot=False)\n",
        "train = mnist.test\n",
        "images, labels = train.images, train.labels\n",
        "only_zero_index,only_one_index,only_two_index = np.where(labels==0)[0],np.where(labels==1)[0],np.where(labels==2)[0]\n",
        "only_zero_image,only_zero_label = images[[only_zero_index]],np.expand_dims(labels[[only_zero_index]],axis=1)\n",
        "only_one_image,only_one_label   = images[[only_one_index]],np.expand_dims(labels[[only_one_index]],axis=1)\n",
        "only_two_image,only_two_label   = images[[only_two_index]],np.expand_dims(labels[[only_two_index]],axis=1)\n",
        "\n",
        "\n",
        "images = np.vstack((only_zero_image,only_one_image))\n",
        "labels = np.vstack((only_zero_label,only_one_label))\n",
        "images,label = shuffle(images,labels)\n",
        "\n",
        "test_image_num = 20\n",
        "testing_images, testing_lables =images[:test_image_num,:],label[:test_image_num,:]\n",
        "training_images,training_lables =images[test_image_num:test_image_num+100,:],label[test_image_num:test_image_num+100,:]\n",
        "\n",
        "#surprise set, only 1 and 2\n",
        "images_surprise = np.vstack((only_one_image,only_two_image))\n",
        "labels_surprise = np.vstack((only_one_label,only_two_label))\n",
        "images_surprise,label_surprise = shuffle(images_surprise,labels_surprise)\n",
        "\n",
        "training_images_surprise,training_lables_surprise =images_surprise[test_image_num:test_image_num+100,:],label[test_image_num:test_image_num+100,:]\n",
        "\n",
        "\n",
        "# 1. Declare the Hidden States and weights and hyper parameters\n",
        "num_epoch = 100\n",
        "h = np.zeros((images.shape[1],3))\n",
        "wx = np.random.randn(784,784)  * 0.2\n",
        "w_rec = np.random.randn(784,784)  * 0.2\n",
        "w_fc = np.random.randn(784,1)* 0.2\n",
        "\n",
        "w_sg_1 = np.random.randn(784,784)* 0.2\n",
        "w_sg_2 = np.random.randn(784,784)* 0.2\n",
        "\n",
        "lr_wx = 0.001\n",
        "lr_wrec = 0.00001\n",
        "lr_sg = 0.0001\n",
        "total_cost = 0\n",
        "\n",
        "for iter in range(num_epoch):\n",
        "    if iter > 50:\n",
        "      training_images = training_images_surprise\n",
        "      training_lables = training_lables_surprise\n",
        "\n",
        "    for current_image_index in range(len(training_images)):\n",
        "        \n",
        "        current_image = training_images[current_image_index]\n",
        "        current_label = training_lables[current_image_index]\n",
        "\n",
        "        l1 = h[:,0].dot(w_rec) + current_image.dot(wx)\n",
        "        l1A = tanh(l1)\n",
        "        h[:,1] = l1A\n",
        "\n",
        "        # ----- Time Stamp 1 Syn Grad Update ------\n",
        "        grad_1sg_part_1 = l1A.dot(w_sg_1)\n",
        "        grad_1sg_part_2 = d_tanh(l1)\n",
        "        grad_1sg_part_rec = h[:,0]\n",
        "        grad_1sg_part_x = current_image\n",
        "\n",
        "        grad_1sg_rec = grad_1sg_part_rec.T.dot(grad_1sg_part_1 * grad_1sg_part_2)\n",
        "        grad_1sg_x = grad_1sg_part_x.T.dot(grad_1sg_part_1 * grad_1sg_part_2)\n",
        "        \n",
        "        w_rec = w_rec + lr_wrec * grad_1sg_rec\n",
        "        wx = wx + lr_wrec * grad_1sg_x\n",
        "        grad_true_0 = (grad_1sg_part_1 * grad_1sg_part_2).dot(w_rec.T)\n",
        "        # ----- Time Stamp 1 Syn Grad Update ------\n",
        "        \n",
        "        l2 = h[:,1].dot(w_rec) + current_image.dot(wx)\n",
        "        l2A = tanh(l2)\n",
        "        h[:,2] = l2A\n",
        "\n",
        "        # ----- Time Stamp 2 Syn Grad Update ------\n",
        "        grad_2sg_part_1 = l2A.dot(w_sg_2)\n",
        "        grad_2sg_part_2 = d_tanh(l2)\n",
        "        grad_2sg_part_rec = h[:,1]\n",
        "        grad_2sg_part_x = current_image\n",
        "\n",
        "        grad_2sg_rec = grad_2sg_part_rec.T.dot(grad_2sg_part_1 * grad_2sg_part_2)\n",
        "        grad_2sg_x = grad_2sg_part_x.T.dot(grad_2sg_part_1 * grad_2sg_part_2)\n",
        "        \n",
        "        w_rec = w_rec + lr_wrec * grad_2sg_rec\n",
        "        wx = wx + lr_wrec * grad_2sg_x\n",
        "        grad_true_1_from_2 = (grad_2sg_part_1 * grad_2sg_part_2).dot(w_rec.T)\n",
        "        # ----- Time Stamp 2 Syn Grad Update ------\n",
        "\n",
        "        # ----- Time Stamp 1 True Gradient Update ------\n",
        "        grad_true_1_part_1 = grad_1sg_part_1 - grad_true_1_from_2\n",
        "        grad_true_1_part_2 = h[:,1]\n",
        "        grad_true_1 = np.expand_dims(grad_true_1_part_2,axis=0).T.dot(np.expand_dims(grad_true_1_part_1,axis=0))\n",
        "        w_sg_1 = w_sg_1 - lr_sg * grad_true_1\n",
        "        # ----- Time Stamp 1 True Gradient Update ------\n",
        "\n",
        "        # ----- Fully Connected for Classification ------\n",
        "        l3 = h[:,2].dot(w_fc)\n",
        "        l3A = log(l3)\n",
        "        cost = np.square(l3A - current_label).sum() * 0.5\n",
        "        total_cost = total_cost + cost\n",
        "        # ----------------------------------------------\n",
        "\n",
        "        # ------- FC weight update ---------------------\n",
        "        grad_fc_part_1 = l3A - current_label\n",
        "        grad_fc_part_2 = d_log(l3)\n",
        "        grad_fc_part_3 = h[:,2]\n",
        "        grad_fc = np.expand_dims(grad_fc_part_3,axis=0).T.dot(np.expand_dims((grad_fc_part_1 * grad_fc_part_2),axis=0))\n",
        "        w_fc = w_fc - lr_wx * grad_fc\n",
        "\n",
        "        grad_true_2_from_3 = (grad_fc_part_1 * grad_fc_part_2).dot(w_fc.T)\n",
        "        # ------- FC weight update ---------------------\n",
        "        \n",
        "        # ----- Time Stamp 2 True Gradient Update ------\n",
        "        grad_true_2_part_1 = grad_2sg_part_1 - grad_true_2_from_3\n",
        "        grad_true_2_part_2 = h[:,2]\n",
        "        grad_true_2 = np.expand_dims(grad_true_2_part_2,axis=0).T.dot(np.expand_dims(grad_true_2_part_1,axis=0))\n",
        "        w_sg_2 = w_sg_2 - lr_sg * grad_true_2\n",
        "        # ----- Time Stamp 2 True Gradient Update ------\n",
        "    if iter % 5 == 0:\n",
        "      True_grad_list.append(np.linalg.norm(grad_fc))\n",
        "      True_grad_list.append(np.linalg.norm(grad_true_2))\n",
        "\n",
        "      print(\"current iter : \",iter, \" Synthetic gradient: \",np.linalg.norm(w_sg_1),end='\\n')\n",
        "      print(\"current iter : \",iter, \" True gradient: \",np.linalg.norm(grad_fc),end='\\n')\n",
        "\n",
        "      print(\"current iter : \",iter, \" Synthetic gradient 2: \",np.linalg.norm(w_sg_2),end='\\n')\n",
        "      print(\"current iter : \",iter, \" True gradient 2: \",np.linalg.norm(grad_true_2),end='\\n')\n",
        "\n",
        "      print(\"current iter : \",iter, \" Total current cost: \",total_cost,end='\\n')\n",
        "      \n",
        "      total_cost = 0\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print('\\n-------------------')\n",
        "print(\"Training Done Final Results\")\n",
        "predict = np.array([])\n",
        "for current_image_index in range(len(testing_images)):\n",
        "    current_image = testing_images[current_image_index]\n",
        "\n",
        "    l1 = h[:,0].dot(w_rec) + current_image.dot(wx)\n",
        "    l1A = tanh(l1)\n",
        "    h[:,1] = l1A\n",
        "\n",
        "    l2 = h[:,1].dot(w_rec) + current_image.dot(wx)\n",
        "    l2A = tanh(l2)\n",
        "    h[:,2] = l2A\n",
        "\n",
        "    l3 = h[:,2].dot(w_fc)\n",
        "    l3A = log(l3)\n",
        "\n",
        "    predict = np.append(predict,l3A)\n",
        "\n",
        "for i in range(len(predict)): \n",
        "    print(\"Results : \", predict[i],\"Results Rounded: \", np.round(predict[i]), \" GT: \", testing_lables[i])\n",
        "print('-------------------\\n')\n",
        "# ------------ Normal Gate RNN Train -------\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# -- end code --"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting ../../MNIST_data/train-images-idx3-ubyte.gz\n",
            "Extracting ../../MNIST_data/train-labels-idx1-ubyte.gz\n",
            "Extracting ../../MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting ../../MNIST_data/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:29: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "current iter :  0  Synthetic gradient:  156.21353716750085\n",
            "current iter :  0  True gradient:  2.190943499047299\n",
            "current iter :  0  Synthetic gradient 2:  156.1489999913607\n",
            "current iter :  0  True gradient 2:  2602.993459869278\n",
            "current iter :  0  Total current cost:  23.29516663594166\n",
            "current iter :  5  Synthetic gradient:  155.2899369319825\n",
            "current iter :  5  True gradient:  2.3541827067616796\n",
            "current iter :  5  Synthetic gradient 2:  154.12349690910423\n",
            "current iter :  5  True gradient 2:  1932.014484793519\n",
            "current iter :  5  Total current cost:  81.80237101353912\n",
            "current iter :  10  Synthetic gradient:  154.76797971980636\n",
            "current iter :  10  True gradient:  0.3477374142827947\n",
            "current iter :  10  Synthetic gradient 2:  152.66413782051347\n",
            "current iter :  10  True gradient 2:  1547.9736297174722\n",
            "current iter :  10  Total current cost:  43.67455242549653\n",
            "current iter :  15  Synthetic gradient:  154.4275841997417\n",
            "current iter :  15  True gradient:  0.14022337255800618\n",
            "current iter :  15  Synthetic gradient 2:  151.58640716670925\n",
            "current iter :  15  True gradient 2:  1289.057674144221\n",
            "current iter :  15  Total current cost:  24.194946231824183\n",
            "current iter :  20  Synthetic gradient:  154.16262774036917\n",
            "current iter :  20  True gradient:  0.07301357415568506\n",
            "current iter :  20  Synthetic gradient 2:  150.75214508750125\n",
            "current iter :  20  True gradient 2:  1103.2276809565647\n",
            "current iter :  20  Total current cost:  16.332105768157838\n",
            "current iter :  25  Synthetic gradient:  153.93305959363397\n",
            "current iter :  25  True gradient:  0.03907795048510708\n",
            "current iter :  25  Synthetic gradient 2:  150.07923330835192\n",
            "current iter :  25  True gradient 2:  963.2250558453079\n",
            "current iter :  25  Total current cost:  12.399814571633668\n",
            "current iter :  30  Synthetic gradient:  153.72112242864856\n",
            "current iter :  30  True gradient:  0.0234350955964037\n",
            "current iter :  30  Synthetic gradient 2:  149.5169693862946\n",
            "current iter :  30  True gradient 2:  855.7970459317676\n",
            "current iter :  30  Total current cost:  9.911505535736266\n",
            "current iter :  35  Synthetic gradient:  153.5163700873083\n",
            "current iter :  35  True gradient:  0.015277049566207877\n",
            "current iter :  35  Synthetic gradient 2:  149.02653529601793\n",
            "current iter :  35  True gradient 2:  785.8616515645064\n",
            "current iter :  35  Total current cost:  8.345293447895756\n",
            "current iter :  40  Synthetic gradient:  153.29018043467417\n",
            "current iter :  40  True gradient:  0.011963417667516279\n",
            "current iter :  40  Synthetic gradient 2:  148.54802032460395\n",
            "current iter :  40  True gradient 2:  725.2353866329937\n",
            "current iter :  40  Total current cost:  8.536580674963353\n",
            "current iter :  45  Synthetic gradient:  153.08773091973458\n",
            "current iter :  45  True gradient:  0.01003768300654593\n",
            "current iter :  45  Synthetic gradient 2:  148.12346946239785\n",
            "current iter :  45  True gradient 2:  649.5923896838968\n",
            "current iter :  45  Total current cost:  7.092152354578222\n",
            "current iter :  50  Synthetic gradient:  152.904527789769\n",
            "current iter :  50  True gradient:  0.007845726290876815\n",
            "current iter :  50  Synthetic gradient 2:  147.7514188594448\n",
            "current iter :  50  True gradient 2:  588.1987773696392\n",
            "current iter :  50  Total current cost:  5.755808831678543\n",
            "current iter :  55  Synthetic gradient:  151.98620412302583\n",
            "current iter :  55  True gradient:  2.4135805825837653e-07\n",
            "current iter :  55  Synthetic gradient 2:  146.01157197860232\n",
            "current iter :  55  True gradient 2:  1369.6848663712544\n",
            "current iter :  55  Total current cost:  97.18714369013294\n",
            "current iter :  60  Synthetic gradient:  151.2334061402965\n",
            "current iter :  60  True gradient:  1.1887588845992514\n",
            "current iter :  60  Synthetic gradient 2:  145.66624830254983\n",
            "current iter :  60  True gradient 2:  1025.878783770219\n",
            "current iter :  60  Total current cost:  69.02205456603832\n",
            "current iter :  65  Synthetic gradient:  150.67304589749557\n",
            "current iter :  65  True gradient:  1.0638235499687447\n",
            "current iter :  65  Synthetic gradient 2:  145.4247128393349\n",
            "current iter :  65  True gradient 2:  920.5001444287499\n",
            "current iter :  65  Total current cost:  63.97430852952201\n",
            "current iter :  70  Synthetic gradient:  150.2195327730636\n",
            "current iter :  70  True gradient:  0.9248953085193569\n",
            "current iter :  70  Synthetic gradient 2:  145.22490938652817\n",
            "current iter :  70  True gradient 2:  828.6161106177277\n",
            "current iter :  70  Total current cost:  63.88470903061652\n",
            "current iter :  75  Synthetic gradient:  149.83644666490386\n",
            "current iter :  75  True gradient:  0.7925198603850886\n",
            "current iter :  75  Synthetic gradient 2:  145.05741507669546\n",
            "current iter :  75  True gradient 2:  753.5431265331451\n",
            "current iter :  75  Total current cost:  63.79138033525537\n",
            "current iter :  80  Synthetic gradient:  149.5035787683209\n",
            "current iter :  80  True gradient:  0.655641002320915\n",
            "current iter :  80  Synthetic gradient 2:  144.91628284309058\n",
            "current iter :  80  True gradient 2:  683.4273198991438\n",
            "current iter :  80  Total current cost:  63.78754353733741\n",
            "current iter :  85  Synthetic gradient:  149.20563331305388\n",
            "current iter :  85  True gradient:  0.6989598997775306\n",
            "current iter :  85  Synthetic gradient 2:  144.80483761096352\n",
            "current iter :  85  True gradient 2:  593.975137998668\n",
            "current iter :  85  Total current cost:  64.21357401380477\n",
            "current iter :  90  Synthetic gradient:  148.9318650751274\n",
            "current iter :  90  True gradient:  1.192956186830484\n",
            "current iter :  90  Synthetic gradient 2:  144.72650055553993\n",
            "current iter :  90  True gradient 2:  520.6723974721434\n",
            "current iter :  90  Total current cost:  63.795880242435565\n",
            "current iter :  95  Synthetic gradient:  148.6849878632938\n",
            "current iter :  95  True gradient:  1.2785552599858117\n",
            "current iter :  95  Synthetic gradient 2:  144.66179073170193\n",
            "current iter :  95  True gradient 2:  497.9815727900685\n",
            "current iter :  95  Total current cost:  62.68107073628828\n",
            "\n",
            "-------------------\n",
            "Training Done Final Results\n",
            "Results :  0.6180840252661512 Results Rounded:  1.0  GT:  [0]\n",
            "Results :  0.6410192500558253 Results Rounded:  1.0  GT:  [1]\n",
            "Results :  0.5792926595422887 Results Rounded:  1.0  GT:  [1]\n",
            "Results :  0.5126987285937761 Results Rounded:  1.0  GT:  [1]\n",
            "Results :  0.5690530196992166 Results Rounded:  1.0  GT:  [0]\n",
            "Results :  0.5953686030452086 Results Rounded:  1.0  GT:  [0]\n",
            "Results :  0.7851630098818838 Results Rounded:  1.0  GT:  [1]\n",
            "Results :  0.41983838718515787 Results Rounded:  0.0  GT:  [1]\n",
            "Results :  0.28107011123460646 Results Rounded:  0.0  GT:  [0]\n",
            "Results :  0.42612265605516086 Results Rounded:  0.0  GT:  [0]\n",
            "Results :  0.5417543358217385 Results Rounded:  1.0  GT:  [1]\n",
            "Results :  0.36270851583815766 Results Rounded:  0.0  GT:  [1]\n",
            "Results :  0.5701424513436293 Results Rounded:  1.0  GT:  [1]\n",
            "Results :  0.6671353644886482 Results Rounded:  1.0  GT:  [0]\n",
            "Results :  0.5783966657497489 Results Rounded:  1.0  GT:  [0]\n",
            "Results :  0.5641952199269229 Results Rounded:  1.0  GT:  [0]\n",
            "Results :  0.529245687589303 Results Rounded:  1.0  GT:  [0]\n",
            "Results :  0.1602066816020164 Results Rounded:  0.0  GT:  [0]\n",
            "Results :  0.5865124577631329 Results Rounded:  1.0  GT:  [0]\n",
            "Results :  0.674166659513001 Results Rounded:  1.0  GT:  [0]\n",
            "-------------------\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}